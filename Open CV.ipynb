{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e628c07",
   "metadata": {},
   "source": [
    "# OPENCV TUTORIAL FOR IMAGE AND VIDEO PROCESSING\n",
    "\n",
    "## Loading Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f70bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a50907a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tole 01\\Desktop\\Green_Tech_Innovation\n"
     ]
    }
   ],
   "source": [
    "%cd C:/Users/Tole 01/Desktop/Green_Tech_Innovation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ba1830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 585C-3D72\n",
      "\n",
      " Directory of C:\\Users\\Tole 01\\Desktop\\Green_Tech_Innovation\n",
      "\n",
      "02/27/2023  05:26 PM    <DIR>          .\n",
      "02/24/2023  12:41 PM    <DIR>          ..\n",
      "02/23/2023  11:59 AM            15,625 Desarrollo de Software.docx\n",
      "02/26/2023  05:19 PM        14,707,245 Forest_fire_video_1.mp4\n",
      "02/24/2023  03:11 PM               492 Git_commands.txt\n",
      "02/24/2023  12:56 PM           330,655 Infrared_fire.jpg\n",
      "02/26/2023  05:52 PM            20,346 Open CV.ipynb\n",
      "02/26/2023  12:08 PM           147,501 rotated_ir_fire.jpg\n",
      "02/27/2023  05:23 PM             3,138 tablero_ajedrez1.jpeg\n",
      "02/27/2023  05:23 PM            50,235 tablero_ajedrez3.jpeg\n",
      "               8 File(s)     15,275,237 bytes\n",
      "               2 Dir(s)  119,735,652,352 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58498a",
   "metadata": {},
   "source": [
    "## Load an Image\n",
    "\n",
    "`cv2.imread()` read the image and creates a pixel's array. It supports different extensions:\n",
    "\n",
    "- Windows bitmaps - `*.bmp, *.dib` \n",
    "- JPEG files - `*.jpeg, *.jpg, *.jpe` \n",
    "- JPEG 2000 files - `*.jp2` \n",
    "- Portable Network Graphics - `*.png `\n",
    "- WebP - `*.webp `\n",
    "- Portable image format - `*.pbm, *.pgm, *.ppm *.pxm, *.pnm `\n",
    "- PFM files - `*.pfm `\n",
    "- Sun rasters - `*.sr, *.ras `\n",
    "- TIFF files - `*.tiff, *.tif `\n",
    "- OpenEXR Image files -` *.exr `\n",
    "- Radiance HDR - `*.hdr, *.pic `\n",
    "- Raster and Vector geospatial data supported by GDAL \n",
    "\n",
    "It's saved in **`BGR`**, not in RGB\n",
    "\n",
    "The function **`cv2.imshow()`** displays the image in a box.\n",
    "\n",
    "- **Argument 0:** It's the name of the image box whic It's a **str**.\n",
    "- **Argument 1:** It's the saved image. \n",
    "\n",
    "The function **`cv2.waitKey()`** stimates the time of the display box and wait an infinite time for you to press some key on the keyboard.\n",
    "\n",
    "- **Argument 0:** It's the box's displayed time. It's an **int**. The number 1000 is equivalent to 1 second. 0 is infinite time.  \n",
    "\n",
    "The function **`cv2.destroyAllWindows()`** allows to close the display box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMIMREAD_COLOR is an argument that saves the image in BGR \n",
    "img_rgb = cv2.imread('Infrared_fire.jpg',cv2.IMREAD_COLOR)\n",
    "cv2.imshow('Image1',img_rgb)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d1b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMREAD_GRAYSCALE is an argument that saves the image in a GREY Scale \n",
    "img_gray = cv2.imread('Infrared_fire.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "cv2.imshow('Image1',img_gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMREAD_UNCHANGED is an argument that saves the image as it was saved originally\n",
    "img_withoutChanges = cv2.imread('Infrared_fire.jpg',cv2.IMREAD_UNCHANGED)\n",
    "cv2.imshow('Image1',img_withoutChanges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882b16d",
   "metadata": {},
   "source": [
    "## Changing all Values in a BGR Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fa2717",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = cv2.imread('Infrared_fire.jpg',cv2.IMREAD_UNCHANGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(img2)):\n",
    "    for y in range(len(img2[x])):\n",
    "        img2[x][y][0] = 0 \n",
    "        print(img2[x][y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2.min(), img2.max(), img2.mean(), img2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b03b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's displayed during 5 seconds\n",
    "cv2.imshow('Image1',img2)\n",
    "cv2.waitKey(5000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec40e754",
   "metadata": {},
   "source": [
    "## Image Resize\n",
    "\n",
    "We use the function **`cv2.resize()`** to resize an image in pixels x pixels or by scale lenght.\n",
    "\n",
    "- **Argument 0:** It's the saved image.\n",
    "- **Argument 1:** Size in pixels. Ex: (1,2), 1 pixel x 2 pixel \n",
    "- **Argument 2 & 3:** It works when we place (0,0) in argument 1. Then we place fx= **float** on argument 2 and fy=**float** on argument 3 for resize the image according to the original scale. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ce6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing pixel x pixel\n",
    "sized_img = cv2.imread('Infrared_fire.jpg',cv2.IMREAD_COLOR)\n",
    "sized_img = cv2.resize(sized_img, (400,300))\n",
    "cv2.imshow('Image1',sized_img)\n",
    "cv2.waitKey(5000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc62bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize by scale\n",
    "sized_img = cv2.imread('Infrared_fire.jpg',cv2.IMREAD_COLOR)\n",
    "sized_img = cv2.resize(sized_img, (0,0), fx=0.5, fy=0.3)\n",
    "cv2.imshow('Image1',sized_img)\n",
    "cv2.waitKey(5000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e314538",
   "metadata": {},
   "source": [
    "## Rotating the Image and Saving it\n",
    "\n",
    "The funcion **`cv2.rotate()`** rotates the image.\n",
    "\n",
    "- **Argument 0:** It's the saved image.\n",
    "- **Argument 1:** We place cv2.ROTATE and press tab in order to choose some method for rotating the image. \n",
    "\n",
    "The funcion **`cv2.imwrite()`** saves the image with an specific name.\n",
    "\n",
    "- **Argument 0:** It's the file's name.\n",
    "- **Argument 1:** It's the saved image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82217f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotating the image\n",
    "rotated_img = cv2.imread('Infrared_fire.jpg',cv2.IMREAD_COLOR)\n",
    "rotated_img = cv2.rotate(rotated_img, cv2.ROTATE_90_CLOCKWISE)\n",
    "rotated_img = cv2.resize(rotated_img, (0,0), fx=1.5, fy=0.8)\n",
    "cv2.imshow('Image1',rotated_img)\n",
    "cv2.waitKey(5000)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Saving the image\n",
    "cv2.imwrite('rotated_ir_fire.jpg', rotated_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e596afa",
   "metadata": {},
   "source": [
    "## Copy Rows and Columns from an Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9edd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('Infrared_fire.jpg',cv2.IMREAD_COLOR)\n",
    "\n",
    "# Copy a range of rows, columns from the original image\n",
    "copied_img = img[100:600, 600:1000]\n",
    "\n",
    "cv2.imshow('Image1',copied_img)\n",
    "cv2.waitKey(5000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73f99c",
   "metadata": {},
   "source": [
    "# Video Processing \n",
    "\n",
    "## Video Capture\n",
    "\n",
    "The function **`cv2.VideoCapture()`** will define which video or webcam are gonna be processed.\n",
    "\n",
    "- **Argument 0:** It has 2 options: the 1st one is the video file's path, the 2nd one refers to integers (0,1,2,...), depending on how much devices (webcams) are connected to the computer.\n",
    "\n",
    "To activate the webcams or the video, you must place the **`cap.read()`** function in a while loop, that would break with a certain condition accomplished with **`ord()`**.\n",
    "\n",
    "The **`cap.read()`** function are equal to different variables. The 1st one is `ret` which refers to video issues and the 2nd one is `frame` which gives you the image that will be processed. So, **ret** will tell you if you have problems or bugs in **frame**. \n",
    "\n",
    " **`ord()`** will receive a key from the keyboard to close the program, otherwise the program will execute infinitely. Also, **`cv2.waitKey(1)`** must have int=1 to run the program properly, because it refers to the time space between one frame and another.  \n",
    "\n",
    "**`cap.release()`** will release the camera, preventing it from being used by other programs such as OBS,google meets and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Video frame 1',frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae5e19",
   "metadata": {},
   "source": [
    "## Camera's Array\n",
    "\n",
    "We crate a black image using **`np.zeros(frame.shape, np.uint8)`**. It creates a 0's array whose shape is equal to the frame's shape. The other argument refers to the **`dtype`** or the data type.\n",
    "\n",
    "**`int(cap.get())`** captures a measure from a property. Example: `cap.get(3)` captures frame's width values and `cap.get(4)` captures frame's height values.\n",
    "\n",
    "The numpy array starts on the upper left corner and ends on the lower right corner. So, the positions are the next:\n",
    "\n",
    "- upper left camera  `[:height//2, :width//2]`\n",
    "- lower left camera  `[height//2:, :width//2]`\n",
    "- upper right camera `[:height//2, width//2:]`\n",
    "- lower right camera `[height//2:, width//2:]`\n",
    "\n",
    "*NOTE*: the error `could not broadcast input array from shape (320,240,3) into shape (240,320,3)` implies the image's shape doesn't fit in the array . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec48e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Capture width and height values with get()\n",
    "    width = int(cap.get(3))\n",
    "    height = int(cap.get(4))\n",
    "    \n",
    "    # Create a video box\n",
    "    image = np.zeros(frame.shape, np.uint8)\n",
    "    \n",
    "    # Resize the frame\n",
    "    smaller_frame = cv2.resize(frame, (0,0), fx=0.5, fy=0.5)\n",
    "    \n",
    "    # Frame slicing to place smaller_frames on their corresponding frame's position. You need to resize the ones that are rotates.\n",
    "    image[:height//2, :width//2] = cv2.resize((cv2.rotate(smaller_frame, cv2.ROTATE_90_CLOCKWISE)), (320,240))\n",
    "    image[height//2:, :width//2] = smaller_frame\n",
    "    image[:height//2, width//2:] = smaller_frame\n",
    "    image[height//2:, width//2:] = cv2.resize((cv2.rotate(smaller_frame, cv2.ROTATE_90_COUNTERCLOCKWISE)), (320,240))    \n",
    "    \n",
    "    # Close the box\n",
    "    cv2.imshow('Video frame 1',image)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27aefb8",
   "metadata": {},
   "source": [
    "## Drawing and Writing on the Camera\n",
    "\n",
    "The camera box is described by positions, as well as the numpy array. On this case, the upper left corner is considered the (0,0) position. So, we're gonna draw lines, squares and circles with the corresponding functions **`cv2.line()`**, **`cv2.rectangle()`** and **`cv2.circle()`**\n",
    "\n",
    "**`cv2.line()`** creates a line inside the box. Their arguments are:\n",
    "\n",
    "- **Argument 0:** is the name of the image where the line is going to be displayed.\n",
    "- **Argument 1:** the starting position in standard coordenates format (0,0).\n",
    "- **Argument 2:** the ending position in standard coordenates format (0,0).\n",
    "- **Argument 3:** the circle's color in BGR format (0,0,0). \n",
    "- **Argument 4:** line thickness as an int.\n",
    "\n",
    "**`cv2.rectangle()`** creates a rectangle inside the box. Their arguments are:\n",
    "\n",
    "- **Argument 0:** the name of the image'variable where the line is going to be displayed.\n",
    "- **Argument 1:** the starting position (upper left corner) in standard coordenates format (0,0).\n",
    "- **Argument 2:** the ending position (lower right corner) in standard coordenates format (1,1). \n",
    "- **Argument 3:** the circle's color in BGR format (0,0,0). \n",
    "- **Argument 4:** line thickness as an int.\n",
    "\n",
    "**`cv2.rectangle()`** creates a rectangle inside the box. Their arguments are:\n",
    "\n",
    "- **Argument 0:** the name of the image'variable where the line is going to be displayed.\n",
    "- **Argument 1:** the starting position (center position) in standard coordenates format (0,0).\n",
    "- **Argument 2:** radius size as int. \n",
    "- **Argument 3:** the circle's color in BGR format (0,0,0). \n",
    "- **Argument 4:** line thickness as an int.\n",
    "\n",
    "On the other side, we could write with **`cv2.putText()`** function. \n",
    "\n",
    "**`cv2.putText()`** creates an script over the box. Its arguments are:\n",
    "\n",
    "- **Argument 0:** the name of the image'variable where the line is going to be displayed.\n",
    "- **Argument 1:** the  displayed text.\n",
    "- **Argument 2:** the starting position (center position) in standard coordenates format (0,0).\n",
    "- **Argument 3:** the used font. \n",
    "- **Argument 4:** the font's size as int. \n",
    "- **Argument 5:** the font's color in BGR format (0,0,0). \n",
    "- **Argument 6:** letter's thickness as an int.\n",
    "- **Argument 7:** line type called with cv2.LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5840011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the capture video\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "while True:\n",
    "    # Create the frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Capture width and height values with get()\n",
    "    width = int(cap.get(3))\n",
    "    height = int(cap.get(4))\n",
    "    \n",
    "    # Draw figures\n",
    "    img = cv2.line(frame, (0,0), (width,height), (0,255,0), 5)\n",
    "    img = cv2.rectangle(img, (100,100), (200,200), (255,0,0), 5)\n",
    "    img = cv2.circle(img, (200,200), 20, (0,0,255), 5)\n",
    "    \n",
    "    # Define the font and the text\n",
    "    font = cv2.FONT_ITALIC\n",
    "    img = cv2.putText(img, \"There's fire\", (200, height - 10), font,2, (12,3,163), 5, cv2.LINE_8)\n",
    "    \n",
    "    # Show the camera\n",
    "    cv2.imshow('Camara con dibujos',img)\n",
    "    \n",
    "    # Close the box\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c2aa5",
   "metadata": {},
   "source": [
    "## Mask to Detect Colors\n",
    "\n",
    "The function **`cv2.cvtColor()`** receives our BGR frame as first argument in order to transform it into an HSV (Hue, Saturation, Value) image through the second argument **`cv2.COLOR_BGR2HSV`**. \n",
    "\n",
    "We need to convert the image into HSV format for the function **`cv2.inRange()`** read it. This function has the next arguments:\n",
    "\n",
    "- **Argument 0:** the hsv frame's variable.\n",
    "- **Argument 1:** the minimum BGR values being considered (lower threshold mask) in a numpy array.\n",
    "- **Argument 2:** The maximum BGR values being considered (higher threshold mask) in a numpy array.\n",
    "\n",
    "Finally, we're gonna use the function **`cv2.bitwise_and()`** which declares the next arguments:\n",
    "\n",
    "- **Argument 0:** the frame's variable.\n",
    "- **Argument 1:** the frame's variable..\n",
    "- **Argument 2:** the mask defined by the function inRange.\n",
    "\n",
    "This function receives the same frame twice, so the mask is applied in both frames. Then, both frame's results are combined and are verified by an AND operator. The matched results are gonna be displayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the capture video\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "while True:\n",
    "    # Create the frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Capture width and height values with get()\n",
    "    width = int(cap.get(3))\n",
    "    height = int(cap.get(4))\n",
    "    \n",
    "    # BGR to HSV format\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Create a mask for filtering blue colors\n",
    "    lower_blue = np.array([90, 50, 50])\n",
    "    upper_blue = np.array([130,255,255])\n",
    "    mask = cv2.inRange(hsv,lower_blue,upper_blue)\n",
    "    \n",
    "    # This is an AND operator to compare this mask in both frames.\n",
    "    result = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "    \n",
    "    # Show the camera\n",
    "    cv2.imshow('Filtro para obtener el azul',result)\n",
    "    \n",
    "    # Close the box\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b26a90",
   "metadata": {},
   "source": [
    "### Fire Colors Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff5cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the capture video\n",
    "cap = cv2.VideoCapture('Forest_fire_video_1.mp4')\n",
    "\n",
    "while True:\n",
    "    # Create the frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Capture width and height values with get()\n",
    "    width = int(cap.get(3))\n",
    "    height = int(cap.get(4))\n",
    "    \n",
    "    # BGR to HSV format\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Create a mask for filter fire colors\n",
    "    lower_blue = np.array([0, 0, 128])\n",
    "    upper_blue = np.array([50,255,255])\n",
    "    mask = cv2.inRange(hsv,lower_blue,upper_blue)\n",
    "    \n",
    "    # This is an AND operator to compare this mask in both frames.\n",
    "    result = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "    \n",
    "    # Show the camera\n",
    "    cv2.imshow('Filtro para obtener el rojo',result)\n",
    "    \n",
    "    # Close the box\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216f4f4a",
   "metadata": {},
   "source": [
    "## Corners Detection\n",
    "\n",
    "The function **`cv2.goodFeaturesToTrack()`** receives our **Grey Scaled Image** and finds corners inside of it.This function has the next arguments:\n",
    "\n",
    "- **Argument 0:** the Grey Scaled image's variable.\n",
    "- **Argument 1:** the number of best corners to identify. There could be more corners, but the program will be centrated on the ones that have a better-defined geometry.\n",
    "- **Argument 2:** It must  contain a value between 0.01 and 1. This value stablish how well the corners are detected by the algorythm.\n",
    "- **Argument 3:** the minimum  euclidian distance between corners. *NOTE*: I divided by 2 because it works better.\n",
    "\n",
    "On the other side, **`np.int0()`** transform the float numbers into int.\n",
    "\n",
    "The function **`ravel()`** reduces all array's dimensions to 1. It works on numpy arrays and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "74b04532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Accessing and resizing an image\n",
    "sized_img = cv2.imread('tablero_ajedrez1.jpeg',cv2.IMREAD_COLOR)\n",
    "sized_img = cv2.resize(sized_img, (0,0), fx=2,fy=2)\n",
    "\n",
    "# Convert the image to grey scale\n",
    "greyScale_image = cv2.cvtColor(sized_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Width and height\n",
    "height = int(greyScale_image.shape[0])\n",
    "width = int(greyScale_image.shape[1])\n",
    "\n",
    "# Calculate euclidian distance. I'll use Pythagoras theorem because we have a square.\n",
    "min_euclidian_distance = math.sqrt((float(height)/8)**2 + (float(width)/8)**2)/2\n",
    "\n",
    "# Detecting corners and transforming them into int\n",
    "corners = cv2.goodFeaturesToTrack(greyScale_image, 100, 0.01, min_euclidian_distance)\n",
    "corners = np.int0(corners)\n",
    "\n",
    "# Detecting corners in the original image\n",
    "for corner in corners:\n",
    "    x, y= corner.ravel()\n",
    "    cv2.circle(sized_img, (x,y), 2, (0,0,250), -1)\n",
    "      \n",
    "# Creating lines between every single corner\n",
    "for i in range(len(corners)):\n",
    "    for j in range(i+1,len(corners)):\n",
    "        corner1 = tuple(corners[j][0])\n",
    "        corner2 = tuple(corners[i][0])\n",
    "        \n",
    "        # Random colors for drawinng the lines\n",
    "        color = tuple(map(lambda x: int(x), np.random.randint(0,255, size=3)))\n",
    "        cv2.line(sized_img, corner1, corner2, color, 1)\n",
    "    \n",
    "cv2.imshow('Image1',sized_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dbf3b4",
   "metadata": {},
   "source": [
    "### Corner detection in assymetric corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1da594af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.919195939817225\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Accessing and resizing an image\n",
    "sized_img = cv2.imread('tablero_ajedrez3.jpeg',cv2.IMREAD_COLOR)\n",
    "sized_img = cv2.resize(sized_img, (0,0), fx=0.5,fy=0.5)\n",
    "\n",
    "# Convert the image to grey scale\n",
    "greyScale_image = cv2.cvtColor(sized_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Width and height\n",
    "height = int(greyScale_image.shape[0])\n",
    "width = int(greyScale_image.shape[1])\n",
    "\n",
    "# Calculate euclidian distance. I'll use Pythagoras theorem because we have a square.\n",
    "min_euclidian_distance = math.sqrt((float(height)/16)**2 + (float(width)/16)**2)/1.5\n",
    "print(min_euclidian_distance)\n",
    "\n",
    "# Detecting corners and transforming them into int\n",
    "corners = cv2.goodFeaturesToTrack(greyScale_image, 100, 0.01, min_euclidian_distance)\n",
    "corners = np.int0(corners)\n",
    "\n",
    "# Detecting corners in the original image\n",
    "for corner in corners:\n",
    "    x, y= corner.ravel()\n",
    "    cv2.circle(sized_img, (x,y), 2, (0,0,250), -1)\n",
    "      \n",
    "# Creating lines between every single corner\n",
    "for i in range(len(corners)):\n",
    "    for j in range(i+1,len(corners)):\n",
    "        corner1 = tuple(corners[j][0])\n",
    "        corner2 = tuple(corners[i][0])\n",
    "        \n",
    "        # Random colors for drawinng the lines\n",
    "        color = tuple(map(lambda x: int(x), np.random.randint(0,255, size=3)))\n",
    "        cv2.line(sized_img, corner1, corner2, color, 1)\n",
    "    \n",
    "cv2.imshow('Image1',sized_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf675abe",
   "metadata": {},
   "source": [
    "## Template Image\n",
    "\n",
    "We can use opencv to detect specific objects on an image,but the object we want to capture must have an template  image with the same size as the object on the original image.\n",
    "\n",
    "The template images will be a soccerball and a soccer shoe. So, we're gonna find them on the original image.\n",
    "\n",
    "All the **`methods`** for template matching are called in order to see which one is the better for our purpose.\n",
    "\n",
    "The function **`cv2.matchTemplate()`** makes a convolution, a method that moves the template along the image in order to locate the best place for it. the function calculates how much the template match on every position. The main funtion's arguments are:\n",
    "\n",
    "- **Argument 0:** the image.\n",
    "- **Argument 1:** the object's template.\n",
    "- **Argument 2:** the used method.\n",
    "\n",
    "*NOTE*: the function's is described as (HI - hT + 1, WI - wT + 1) and it gives a different punctuation according to the method and the position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8ea1b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('soccer.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img, (0,0), fx=0.3,fy=0.3)\n",
    "\n",
    "template_ball = cv2.imread('soccerball.jpeg',cv2.IMREAD_GRAYSCALE)\n",
    "template_ball = cv2.resize(template_ball, (0,0), fx=0.3,fy=0.3)\n",
    "h, w = template_ball.shape\n",
    "\n",
    "# Different convolution methods\n",
    "methods = [cv2.TM_CCOEFF, cv2.TM_CCOEFF_NORMED, cv2.TM_CCORR,\n",
    "           cv2.TM_CCORR_NORMED, cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]\n",
    "\n",
    "for method in methods:\n",
    "    # We create an image copy or each method\n",
    "    img2 = img.copy()\n",
    "    \n",
    "    # Doing the Convolution\n",
    "    result= cv2.matchTemplate(img2, template_ball, method)\n",
    "    \n",
    "    # Give me the min and the max values for the mathching their positions.\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "    \n",
    "    # Conditional for the  method SQDIFF. Their minimum values are the better matching places. \n",
    "    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "        location = min_loc\n",
    "    else:\n",
    "        location = max_loc\n",
    "        \n",
    "    # Drawing the rectangle\n",
    "    bottom_right = (location[0] + w, location[1] + h)\n",
    "    cv2.rectangle(img2, location, bottom_right, 255, 5)\n",
    "    \n",
    "    # Displayed images\n",
    "    cv2.imshow('Match',img2)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21b6c8",
   "metadata": {},
   "source": [
    "### Matching soccer shoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "997a88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('soccer.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img, (0,0), fx=0.3,fy=0.3)\n",
    "\n",
    "template_ball = cv2.imread('soccer_shoe.jpeg',cv2.IMREAD_GRAYSCALE)\n",
    "template_ball = cv2.resize(template_ball, (0,0), fx=0.3,fy=0.3)\n",
    "h, w = template_ball.shape\n",
    "\n",
    "# Different convolution methods\n",
    "methods = [cv2.TM_CCOEFF, cv2.TM_CCOEFF_NORMED, cv2.TM_CCORR,\n",
    "           cv2.TM_CCORR_NORMED, cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]\n",
    "\n",
    "for method in methods:\n",
    "    # We create an image copy or each method\n",
    "    img2 = img.copy()\n",
    "    \n",
    "    # Doing the Convolution\n",
    "    result= cv2.matchTemplate(img2, template_ball, method)\n",
    "    \n",
    "    # Give me the min and the max values for the mathching their positions.\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "    \n",
    "    # Conditional for the  method SQDIFF. Their minimum values are the better matching places. \n",
    "    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "        location = min_loc\n",
    "    else:\n",
    "        location = max_loc\n",
    "        \n",
    "    # Drawing the rectangle\n",
    "    bottom_right = (location[0] + w, location[1] + h)\n",
    "    cv2.rectangle(img2, location, bottom_right, 255, 5)\n",
    "    \n",
    "    # Displayed images\n",
    "    cv2.imshow('Match',img2)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9463fc7",
   "metadata": {},
   "source": [
    "## Face and Eye Detection\n",
    "\n",
    "We're gonna use **`Haar Cascade`** to realize this detection. Is a pre-trained classifier to identify specific features from faces.\n",
    "\n",
    "On the function **`cv2.CascadeClassifier()`** you're gonna pass the next arguments in ordar to call the trained model:\n",
    "\n",
    "- **Argument 0:** the paht of the haarcascades.\n",
    "- **Argument 1:** all face's data for haarcascade.\n",
    "- **Argument 2:** the used method.\n",
    "\n",
    "Then will stablish model's parameters for face detection through the function **`face_cascade.detectMultiScale`**, whose arguments are:\n",
    "\n",
    "- **Argument 0:** **`scaleFactor`** is a parameter that specifies how much the image size is reduced at each image scale.                                                                                                                                                                                          Basically the scale factor is used to create your scale pyramid. More explanation can be found here. In short, as described here, your model has a fixed size defined during training, which is visible in the xml. This means that this size of face is detected in the image if present. However, by rescaling the input image, you can resize a larger face to a smaller one, making it detectable by the algorithm.                                                                                                     1.05 is a good possible value for this, which means you use a small step for resizing, i.e. reduce size by 5%, you increase the chance of a matching size with the model for detection is found. This also means that the algorithm works slower since it is more thorough. You may increase it to as much as 1.4 for faster detection, with the risk of missing some faces altogether.\n",
    "\n",
    "- **Argument 1:** **`minNeighbors`** Parameter specifying how many neighbors each candidate rectangle should have to retain it.                                                                                                                                                                                    This parameter will affect the quality of the detected faces. Higher value results in less detections but with higher quality. 3~6 is a good value for it.\n",
    "\n",
    "- **Argument 2:** **`minSize`** is the minimum possible object size. Objects smaller than that are ignored.                                                                                                                                                                                                      This parameter determine how small size you want to detect. You decide it! Usually, [30, 30] is a good start for face detection.\n",
    "\n",
    "- **Argument 3:** **`maxSize`** is the Maximum possible object size. Objects bigger than this are ignored.                                                                                                This parameter determine how big size you want to detect. Again, you decide it! Usually, you don't need to set it manually, the default value assumes you want to detect without an upper limit on the size of the face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "aa39440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "while True:\n",
    "    # Define frame and convert it to gray scale\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Parameters for the Haar Model\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    # Creating a rectangle for faces\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), [0,0,255], 5)\n",
    "        \n",
    "        # Face zone for detecting eyes later\n",
    "        roi_gray = gray[y:y+w, x:x+w]\n",
    "        roi_color = frame[y:y+w, x:x+w]\n",
    "        \n",
    "        # Parameters for eye detection\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, 1.3, 5)\n",
    "        \n",
    "        # Creating a rectangle for faces\n",
    "        #for (ex,ey,ew,eh) in eyes:\n",
    "        #    cv2.rectangle(roi_color, (ex,ey), (ex+ew,ey+eh), [204,204,255], 5)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('Video frame 1',frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
